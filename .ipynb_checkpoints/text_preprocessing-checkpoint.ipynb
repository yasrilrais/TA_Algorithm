{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "liberal-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "finished-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessing:\n",
    "    def __init__(self, text=\"test\"):\n",
    "        self.text = text\n",
    "\n",
    "    def lowercase(self):\n",
    "        \"\"\"Convert to lowercase\"\"\"\n",
    "        self.text = self.text.lower()\n",
    "        self.text = self.text.strip()\n",
    "        return self\n",
    "\n",
    "    def strip_html(self):\n",
    "        \"\"\"Stopword removal\"\"\"\n",
    "        soup = BeautifulSoup(self.text, \"lxml\")\n",
    "        self.text = soup.get_text()\n",
    "        return self\n",
    "\n",
    "    def remove_url(self):\n",
    "        \"\"\"Remove URL (http/https/www) or custom URL\"\"\"\n",
    "        self.text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", self.text)\n",
    "        self.text = re.sub(r\"pic.twitter.com\\S+\", \"\", self.text)  # custom for twitter\n",
    "        return self\n",
    "\n",
    "    def remove_email(self):\n",
    "        \"\"\"Remove email\"\"\"\n",
    "        self.text = re.sub(\"\\S*@\\S*\\s?\", \"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_between_square_brackets(self):\n",
    "        \"\"\"Remove string beetwen square brackets []\"\"\"\n",
    "        self.text = re.sub(\"\\[[^]]*\\]\", \"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_numbers(self):\n",
    "        \"\"\"Remove numbers\"\"\"\n",
    "        self.text = re.sub(\"[-+]?[0-9]+\", \"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_emoji(self):\n",
    "        \"\"\"Remove emoji, e.g ðŸ˜œðŸ˜€ \"\"\"\n",
    "        emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        self.text = emoji_pattern.sub(r\"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_emoticon(self):\n",
    "        \"\"\"Remove emoticon, e.g :-)\"\"\"\n",
    "        emoticon_pattern = re.compile(u\"(\" + u\"|\".join(k for k in EMOTICONS) + u\")\")\n",
    "        self.text = emoticon_pattern.sub(r\"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def convert_emoji(self):\n",
    "        \"\"\"Convert emoji to word\"\"\"\n",
    "        for emoji in UNICODE_EMO:\n",
    "            self.text = self.text.replace(\n",
    "                emoji,\n",
    "                \"_\".join(UNICODE_EMO[emoji].replace(\",\", \"\").replace(\":\", \"\").split()),\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def convert_emoticon(self):\n",
    "        \"\"\"Convert emoticon to word\"\"\"\n",
    "        for emoticon in EMOTICONS:\n",
    "            self.text = re.sub(\n",
    "                u\"(\" + emoticon + \")\",\n",
    "                \"_\".join(EMOTICONS[emoticon].replace(\",\", \"\").split()),\n",
    "                self.text,\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def remove_punctuation(self):\n",
    "        \"\"\"Remove punctuation\"\"\"\n",
    "        self.text = re.sub(r\"[^\\w\\s]\", \"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_non_ascii(self):\n",
    "        \"\"\"Remove non-ascii character\"\"\"\n",
    "        self.text = (\n",
    "            unicodedata.normalize(\"NFKD\", self.text)\n",
    "            .encode(\"ascii\", \"ignore\")\n",
    "            .decode(\"utf-8\", \"ignore\")\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def normalize_word(self):\n",
    "        \"\"\"Normalize slang world\"\"\"\n",
    "        normal_word_path = pd.read_csv(\"C:/Users/ASUS/TA01/key_norm.csv\")\n",
    "\n",
    "        self.text = \" \".join(\n",
    "            [\n",
    "                normal_word_path[normal_word_path[\"singkat\"] == word][\"hasil\"].values[0]\n",
    "                if (normal_word_path[\"singkat\"] == word).any()\n",
    "                else word\n",
    "                for word in self.text.split()\n",
    "            ]\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def stemming(self):\n",
    "        \"\"\"Stemming for Bahasa with Sastrawi\"\"\"\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "\n",
    "        self.text = stemmer.stem(self.text)\n",
    "        return self\n",
    "\n",
    "    def tokenize(self):\n",
    "        \"\"\"Tokenize words\"\"\"\n",
    "        self.words = nltk.word_tokenize(self.text)\n",
    "        return self\n",
    "\n",
    "    def stopwords_removal(self):\n",
    "        \"\"\"Stopword removal\"\"\"\n",
    "        stopword = stopwords.words(\"indonesian\")\n",
    "        more_stopword = [\n",
    "            \"assalamualaikum\", \"wr\", \"wb\", \"pak\",\n",
    "            \"bu\", \"selamat\", \"siang\", \"pagi\",\n",
    "            \"sore\", \"malam\", \"saya\",\n",
    "            \"terimakasih\", \"terima\",\n",
    "            \"kasih\", \"kepada\", \"bpk\",\n",
    "            \"ibu\", \"mohon\", \"tolong\",\n",
    "            \"maaf\", \"dear\", \"wassalamualaikum\", \"regards\", ]  # add more stopword to default corpus\n",
    "        stop_factory = stopword + more_stopword\n",
    "\n",
    "        clean_words = []\n",
    "        for word in self.words:\n",
    "            if word not in stop_factory:\n",
    "                clean_words.append(word)\n",
    "        self.words = clean_words\n",
    "        return self\n",
    "\n",
    "    def join_words(self):\n",
    "        \"\"\"Jonin all words\"\"\"\n",
    "        self.words = \" \".join(self.words)\n",
    "        return self\n",
    "\n",
    "    def do_all(self, text):\n",
    "        \"\"\"Do all text preprocessing process\"\"\"  # or custom process\n",
    "        self.text = text\n",
    "        self = self.lowercase()\n",
    "        self = self.strip_html()\n",
    "        self = self.remove_url()\n",
    "        self = self.remove_email()\n",
    "        self = self.remove_between_square_brackets()\n",
    "        self = self.remove_numbers()\n",
    "        self = self.remove_emoticon()\n",
    "        self = self.remove_emoji()\n",
    "        self = self.convert_emoticon()\n",
    "        self = self.convert_emoji()\n",
    "        self = self.remove_punctuation()\n",
    "        self = self.remove_non_ascii()\n",
    "        self = self.normalize_word()\n",
    "        self = self.stemming()\n",
    "        self = self.tokenize()\n",
    "        self = self.stopwords_removal()\n",
    "        self = self.join_words()\n",
    "        return self.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "meaning-extreme",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keluhan</th>\n",
       "      <th>bagian</th>\n",
       "      <th>id</th>\n",
       "      <th>keluhan_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear BAA,Mohon di bantu untuk merubah satatus ...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear BAA Telkom University,\\nSaya sebagai sala...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mohon maaf saya haidar mau komplain pada saat ...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>213.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>permisi saya mau komplain, biaya pendidikan sa...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>186.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Assalamualaikum wr wb.Maaf pak, saya sudah me...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Assalamualaikum pak/bu, saya Rizqillah Zahra (...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>443.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"Saya lupa melakukan cetak nilai akhir (KHS), ...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>saya ingin mencetak transkip nilai akhir tapi ...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Selamat siang, ;\\nSaya Aulia Fiya Maulida Mah...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>563.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>saya ingin mencetak transkip nilai akhir tapi ...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             keluhan bagian  id  \\\n",
       "0  Dear BAA,Mohon di bantu untuk merubah satatus ...    BAA   1   \n",
       "1  Dear BAA Telkom University,\\nSaya sebagai sala...    BAA   1   \n",
       "2  Mohon maaf saya haidar mau komplain pada saat ...    BAA   1   \n",
       "3  permisi saya mau komplain, biaya pendidikan sa...    BAA   1   \n",
       "4  \"Assalamualaikum wr wb.Maaf pak, saya sudah me...    BAA   1   \n",
       "5  Assalamualaikum pak/bu, saya Rizqillah Zahra (...    BAA   1   \n",
       "6  \"Saya lupa melakukan cetak nilai akhir (KHS), ...    BAA   1   \n",
       "7  saya ingin mencetak transkip nilai akhir tapi ...    BAA   1   \n",
       "8  \"Selamat siang, ;\\nSaya Aulia Fiya Maulida Mah...    BAA   1   \n",
       "9  saya ingin mencetak transkip nilai akhir tapi ...    BAA   1   \n",
       "\n",
       "   keluhan_length  \n",
       "0           254.0  \n",
       "1           202.0  \n",
       "2           213.0  \n",
       "3           186.0  \n",
       "4           366.0  \n",
       "5           443.0  \n",
       "6           180.0  \n",
       "7           111.0  \n",
       "8           563.0  \n",
       "9           111.0  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'C:/Users/ASUS/TA01/01_pickle/01_data_training.pickle'\n",
    "\n",
    "with open(data_path, 'rb') as data_training:\n",
    "    data = pickle.load(data_training)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "modern-olive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing failed !\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "tp = TextPreprocessing() # load module text preprocessing\n",
    "\n",
    "def dask_this(data):\n",
    "    data['clean_keluhan'] = data['keluhan'].apply(tp.do_all)\n",
    "    return data\n",
    "\n",
    "ddata = dd.from_pandas(data, npartitions=10)\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    data = ddata.map_partitions(dask_this).compute(scheduler='processes', num_workers=10)\n",
    "except:\n",
    "    print('Text preprocessing failed !')\n",
    "else:\n",
    "    data.to_csv('C:/Users/ASUS/TA01/clean_data_training.csv', encoding='utf-8')\n",
    "    print('Text preprocessing success !')\n",
    "    print('Elapsed time:', time.time() - start_time, 'seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
