{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "liberal-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "finished-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessing:\n",
    "    def __init__(self, text=\"test\"):\n",
    "        self.text = text\n",
    "\n",
    "    def lowercase(self):\n",
    "        \"\"\"Convert to lowercase\"\"\"\n",
    "        self.text = str(self.text).lower()\n",
    "        self.text = self.text.strip()\n",
    "        return self\n",
    "\n",
    "    def strip_html(self):\n",
    "        \"\"\"Stopword removal\"\"\"\n",
    "        soup = BeautifulSoup(self.text, \"html.parser\")\n",
    "        self.text = soup.get_text()\n",
    "        return self\n",
    "\n",
    "    def remove_url(self):\n",
    "        \"\"\"Remove URL (http/https/www) or custom URL\"\"\"\n",
    "        self.text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_email(self):\n",
    "        \"\"\"Remove email\"\"\"\n",
    "        self.text = re.sub(\"\\S*@\\S*\\s?\", \"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_between_square_brackets(self):\n",
    "        \"\"\"Remove string beetwen square brackets []\"\"\"\n",
    "        self.text = re.sub(\"\\[[^]]*\\]\", \"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_numbers(self):\n",
    "        \"\"\"Remove numbers\"\"\"\n",
    "        self.text = re.sub(\"[-+]?[0-9]+\", \"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_emoji(self):\n",
    "        \"\"\"Remove emoji, e.g ðŸ˜œðŸ˜€ \"\"\"\n",
    "        emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        self.text = emoji_pattern.sub(r\"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_emoticon(self):\n",
    "        \"\"\"Remove emoticon, e.g :-)\"\"\"\n",
    "        emoticon_pattern = re.compile(u\"(\" + u\"|\".join(k for k in EMOTICONS) + u\")\")\n",
    "        self.text = emoticon_pattern.sub(r\"\", self.text)\n",
    "        return self\n",
    "    \n",
    "    def convert_emoji(self):\n",
    "        \"\"\"Convert emoji to word\"\"\"\n",
    "        for emoji in UNICODE_EMO:\n",
    "            self.text = self.text.replace(\n",
    "                emoji,\n",
    "                \"_\".join(UNICODE_EMO[emoji].replace(\",\", \"\").replace(\":\", \"\").split()),\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def convert_emoticon(self):\n",
    "        \"\"\"Convert emoticon to word\"\"\"\n",
    "        for emoticon in EMOTICONS:\n",
    "            self.text = re.sub(\n",
    "                u\"(\" + emoticon + \")\",\n",
    "                \"_\".join(EMOTICONS[emoticon].replace(\",\", \"\").split()),\n",
    "                self.text,\n",
    "            )\n",
    "        return self\n",
    "\n",
    "    def remove_punctuation(self):\n",
    "        \"\"\"Remove punctuation\"\"\"\n",
    "        self.text = re.sub(r\"[^\\w\\s]\", \"\", self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_non_ascii(self):\n",
    "        \"\"\"Remove non-ascii character\"\"\"\n",
    "        self.text = (\n",
    "            unicodedata.normalize(\"NFKD\", self.text)\n",
    "            .encode(\"ascii\", \"ignore\")\n",
    "            .decode(\"utf-8\", \"ignore\")\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def normalize_word(self):\n",
    "        \"\"\"Normalize slang world\"\"\"\n",
    "        normal_word_path = pd.read_csv(\"C:/Users/ASUS/TA01/00_data/key_norm.csv\")\n",
    "\n",
    "        self.text = \" \".join(\n",
    "            [\n",
    "                normal_word_path[normal_word_path[\"singkat\"] == word][\"hasil\"].values[0]\n",
    "                if (normal_word_path[\"singkat\"] == word).any()\n",
    "                else word\n",
    "                for word in self.text.split()\n",
    "            ]\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def stemming(self):\n",
    "        \"\"\"Stemming for Bahasa with Sastrawi\"\"\"\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer()\n",
    "\n",
    "        self.text = stemmer.stem(self.text)\n",
    "        return self\n",
    "\n",
    "    def tokenize(self):\n",
    "        \"\"\"Tokenize words\"\"\"\n",
    "        self.words = nltk.word_tokenize(self.text)\n",
    "        return self\n",
    "\n",
    "    def stopwords_removal(self):\n",
    "        \"\"\"Stopword removal\"\"\"\n",
    "        stopword = stopwords.words(\"indonesian\")\n",
    "        more_stopword = [\n",
    "            \"assalamualaikum\", \"wr\", \"wb\", \"pak\",\n",
    "            \"bu\", \"selamat\", \"siang\", \"pagi\",\n",
    "            \"sore\", \"malam\", \"saya\",\n",
    "            \"terimakasih\", \"terima\",\n",
    "            \"kasih\", \"kepada\", \"bpk\",\n",
    "            \"ibu\", \"mohon\", \"tolong\",\n",
    "            \"maaf\", \"dear\", \"wassalamualaikum\", \"regards\", ]  # add more stopword to default corpus\n",
    "        stop_factory = stopword + more_stopword\n",
    "        stop_factory.remove('tak')\n",
    "        \n",
    "        clean_words = []\n",
    "        for word in self.words:\n",
    "            if word not in stop_factory:\n",
    "                clean_words.append(word)\n",
    "        self.words = clean_words  \n",
    "        return self\n",
    "\n",
    "    def join_words(self):\n",
    "        \"\"\"Jonin all words\"\"\"\n",
    "        self.words = \" \".join(self.words)\n",
    "        return self\n",
    "    \n",
    "    def do_all(self, text):\n",
    "        \"\"\"Do all text preprocessing process\"\"\"  # or custom process\n",
    "        self.text = text\n",
    "        self = self.lowercase()\n",
    "        self = self.strip_html()\n",
    "        self = self.remove_url()\n",
    "        self = self.remove_email()\n",
    "        self = self.remove_between_square_brackets()\n",
    "        self = self.remove_numbers()\n",
    "        self = self.remove_emoticon()\n",
    "        self = self.remove_emoji()\n",
    "        self = self.convert_emoticon()\n",
    "        self = self.convert_emoji()\n",
    "        self = self.remove_punctuation()\n",
    "        self = self.remove_non_ascii()\n",
    "        self = self.normalize_word()\n",
    "        self = self.stemming()\n",
    "        self = self.tokenize()\n",
    "        self = self.stopwords_removal()\n",
    "        self = self.join_words()\n",
    "        return self.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "meaning-extreme",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keluhan</th>\n",
       "      <th>bagian</th>\n",
       "      <th>id</th>\n",
       "      <th>keluhan_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear BAA Mohon di bantu untuk merubah status a...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear BAA Telkom University,\\nSaya sebagai sala...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mohon maaf saya haidar mau komplain pada saat ...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>permisi saya mau komplain, biaya pendidikan sa...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Assalamualaikum wr wb.Maaf pak, saya sudah me...</td>\n",
       "      <td>BAA</td>\n",
       "      <td>1</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>Assalamualaikum Wr. Wb Mohon maaf sebelumnya, ...</td>\n",
       "      <td>LABORAN</td>\n",
       "      <td>1</td>\n",
       "      <td>957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>gaji asisten untuk FRI tidak sebanding (jika d...</td>\n",
       "      <td>LABORAN</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>Selamat siang, maaf mangganggu. Saya Yusrin da...</td>\n",
       "      <td>LABORAN</td>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>maaf sebelumnya, saya olyvia fransiska dari te...</td>\n",
       "      <td>LABORAN</td>\n",
       "      <td>1</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>alat alat di laboratorium fisdas sudah banyak ...</td>\n",
       "      <td>LABORAN</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>793 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               keluhan   bagian  id  \\\n",
       "0    Dear BAA Mohon di bantu untuk merubah status a...      BAA   1   \n",
       "1    Dear BAA Telkom University,\\nSaya sebagai sala...      BAA   1   \n",
       "2    Mohon maaf saya haidar mau komplain pada saat ...      BAA   1   \n",
       "3    permisi saya mau komplain, biaya pendidikan sa...      BAA   1   \n",
       "4    \"Assalamualaikum wr wb.Maaf pak, saya sudah me...      BAA   1   \n",
       "..                                                 ...      ...  ..   \n",
       "788  Assalamualaikum Wr. Wb Mohon maaf sebelumnya, ...  LABORAN   1   \n",
       "789  gaji asisten untuk FRI tidak sebanding (jika d...  LABORAN   1   \n",
       "790  Selamat siang, maaf mangganggu. Saya Yusrin da...  LABORAN   1   \n",
       "791  maaf sebelumnya, saya olyvia fransiska dari te...  LABORAN   1   \n",
       "792  alat alat di laboratorium fisdas sudah banyak ...  LABORAN   1   \n",
       "\n",
       "     keluhan_length  \n",
       "0               253  \n",
       "1               202  \n",
       "2               213  \n",
       "3               186  \n",
       "4               366  \n",
       "..              ...  \n",
       "788             957  \n",
       "789              71  \n",
       "790             184  \n",
       "791             222  \n",
       "792              69  \n",
       "\n",
       "[793 rows x 4 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'C:/Users/ASUS/TA01/01_data_analysis/01_pickle/01_data_training.pickle'\n",
    "\n",
    "with open(data_path, 'rb') as data_training:\n",
    "    data = pickle.load(data_training)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "completed-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TextPreprocessing() # load module text preprocessing\n",
    "\n",
    "data['clean_keluhan'] = data['keluhan'].apply(tp.do_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "modern-olive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing success !\n",
      "Elapsed time: 38.818790912628174 seconds\n",
      "\n",
      "Finish\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import dask.dataframe as dd\n",
    "from dask.multiprocessing import get\n",
    "\n",
    "tp = TextPreprocessing() # load module text preprocessing\n",
    "\n",
    "def dask_this(data):\n",
    "    data['clean_keluhan'] = data['keluhan'].apply(tp.do_all)\n",
    "    return data\n",
    "\n",
    "ddata = dd.from_pandas(data, npartitions=10)\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    data = ddata.map_partitions(dask_this).compute(scheduler='processes', num_workers=10)\n",
    "except:\n",
    "    print('Text preprocessing failed !')\n",
    "else:\n",
    "    data.to_csv('C:/Users/ASUS/TA01/00_data/clean_data_training.csv', encoding='utf-8')\n",
    "    print('Text preprocessing success !')\n",
    "    print('Elapsed time:', time.time() - start_time, 'seconds')\n",
    "finally:\n",
    "    print('\\nFinish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "educated-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['clean_keluhan', 'bagian']\n",
    "data = data[columns]\n",
    "\n",
    "with open('02_pickle/02_clean_data.pickle', 'wb') as output:\n",
    "    pickle.dump(data, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "written-notion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_keluhan</th>\n",
       "      <th>bagian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baa bantu rubah status akademik an rezza rijki...</td>\n",
       "      <td>BAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baa telkom university salah alumni lihat data ...</td>\n",
       "      <td>BAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>haidar komplain bayar bank bni bank milik tera...</td>\n",
       "      <td>BAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>permisi komplain biaya didik lunas status biay...</td>\n",
       "      <td>BAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wbmaaf cancel ksm dosen wali salah input mk am...</td>\n",
       "      <td>BAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>fakultas ilmu terap liburtanggal merah weekend...</td>\n",
       "      <td>LABORAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>gaji asisten fri banding dibandingin fakultas</td>\n",
       "      <td>LABORAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>mangganggu yusrin lab magics fte lampu toilet ...</td>\n",
       "      <td>LABORAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>olyvia fransiska teknik telekomunikasi keluh u...</td>\n",
       "      <td>LABORAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>alat alat laboratorium fisdas pakai</td>\n",
       "      <td>LABORAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>793 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         clean_keluhan   bagian\n",
       "0    baa bantu rubah status akademik an rezza rijki...      BAA\n",
       "1    baa telkom university salah alumni lihat data ...      BAA\n",
       "2    haidar komplain bayar bank bni bank milik tera...      BAA\n",
       "3    permisi komplain biaya didik lunas status biay...      BAA\n",
       "4    wbmaaf cancel ksm dosen wali salah input mk am...      BAA\n",
       "..                                                 ...      ...\n",
       "788  fakultas ilmu terap liburtanggal merah weekend...  LABORAN\n",
       "789      gaji asisten fri banding dibandingin fakultas  LABORAN\n",
       "790  mangganggu yusrin lab magics fte lampu toilet ...  LABORAN\n",
       "791  olyvia fransiska teknik telekomunikasi keluh u...  LABORAN\n",
       "792                alat alat laboratorium fisdas pakai  LABORAN\n",
       "\n",
       "[793 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
